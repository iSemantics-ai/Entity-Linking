{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2NvUDmvuyjIh",
    "outputId": "56bcbe8d-e2e4-429c-dc6d-a7ec7d0ecbe1"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "# import numpy as np\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# Get the absolute path of the current script\n",
    "script_path = Path.cwd()\n",
    "# Get the directory containing the script\n",
    "src_dir = script_path.parent\n",
    "\n",
    "sys.path.append(str(src_dir))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_path = src_dir / 'data/OLD-sp-company-data'\n",
    "filtered_name = 'filtered_ciqcompany'\n",
    "filtered_norm_path = src_dir / 'data/OLD-sp-company-data-normalized'\n",
    "filtered_norm_name = 'normalized'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Num of files: 38\n"
     ]
    }
   ],
   "source": [
    "from matcher.utils import read_files\n",
    "\n",
    "col_names = ['companyid','companyname']\n",
    "all_filtered_norm = read_files(filtered_norm_path, filtered_norm_name, col_names, sep='\\t')\n",
    "# all_filtered_source = read_files(filtered_path, filtered_name, col_names)\n",
    "# all_filtered_source = all_filtered_source.astype({'companyid': 'int64'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1cbd95fedfd429b9427e6b638e71ebe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c24faee9afd402badd5a48615229d53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13a9d6758386479bad644198a917e6df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0f728fa1c0341ceb8459bd2f2c0cef6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46d7ff09d3ef43bab2db6f43c0910d33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6469f49debdf4af88d1fa4e9d83b00f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/86.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matcher.core import SimCSE_Matcher\n",
    "matcher = SimCSE_Matcher(\"sentence-transformers/all-MiniLM-L6-v2\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pinecone-client\n",
      "  Downloading pinecone_client-2.2.2-py3-none-any.whl (179 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.1/179.1 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.22.0 in /usr/local/lib/python3.9/dist-packages (from pinecone-client) (1.23.1)\n",
      "Collecting tqdm>=4.64.1\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.9/dist-packages (from pinecone-client) (2.8.2)\n",
      "Requirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from pinecone-client) (1.26.10)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.9/dist-packages (from pinecone-client) (4.3.0)\n",
      "Requirement already satisfied: pyyaml>=5.4 in /usr/local/lib/python3.9/dist-packages (from pinecone-client) (5.4.1)\n",
      "Collecting dnspython>=2.0.0\n",
      "  Downloading dnspython-2.4.2-py3-none-any.whl (300 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.4/300.4 kB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from pinecone-client) (2.28.1)\n",
      "Collecting loguru>=0.5.0\n",
      "  Downloading loguru-0.7.0-py3-none-any.whl (59 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.5.3->pinecone-client) (1.14.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.19.0->pinecone-client) (2019.11.28)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.19.0->pinecone-client) (2.8)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->pinecone-client) (2.1.0)\n",
      "Installing collected packages: tqdm, loguru, dnspython, pinecone-client\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.64.0\n",
      "    Uninstalling tqdm-4.64.0:\n",
      "      Successfully uninstalled tqdm-4.64.0\n",
      "Successfully installed dnspython-2.4.2 loguru-0.7.0 pinecone-client-2.2.2 tqdm-4.66.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pinecone-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone      \n",
    "\n",
    "pinecone.init(      \n",
    "    api_key='4c1aef96-94be-4f75-8bec-b03058e896fa',      \n",
    "    environment='us-west1-gcp'      \n",
    ")      \n",
    "index = pinecone.Index('inferess-name-links')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = all_filtered_norm[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:03<00:00, 51.38it/s]\n"
     ]
    }
   ],
   "source": [
    "embeddings = matcher.encode(sample['companyname'].tolist(), batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "upsert_values = []\n",
    "for i,row in sample.iterrows():\n",
    "    print(i)\n",
    "    break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upsert_response = index.upsert(\n",
    "    vectors=[\n",
    "        {'id': 'vec1',\n",
    "         'values': [0.1, 0.2, 0.3],\n",
    "         'metadata': {'genre': 'drama'},\n",
    "         'sparse_values': {\n",
    "             'indices': [10, 45, 16],\n",
    "             'values': [0.5, 0.5, 0.2]\n",
    "         }},\n",
    "        {'id': 'vec2',\n",
    "         'values': [0.2, 0.3, 0.4],\n",
    "         'metadata': {'genre': 'action'},\n",
    "         'sparse_values': {\n",
    "             'indices': [15, 40, 11],\n",
    "             'values': [0.4, 0.5, 0.2]\n",
    "         }}\n",
    "    ],\n",
    "    namespace='example-namespace'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a neural network to encode enities into dense dimension represent the charaters that match two entities# Create an dataset fot entity linking \n",
    "\n",
    "1. Collect positive links: \n",
    "`Collect highly scored names with the rule based methods that guarantee correctness of the matching`\n",
    "---\n",
    "2. Collect hard negative links:\n",
    "`Hard negative matches means pretty close but not match to distinguish the distinct point that match between two names`\n",
    "---\n",
    "3. Symbol and what it stand for: \n",
    "`This hopefully will allow the neural networt to understand if some words can match it's intials`\n",
    "---\n",
    "4. Leverage the SME knowledge base to create some fake examples: \n",
    "`Searching for hard patterns like:`\n",
    "- two words have no space between them\n",
    "- some symbols used as short cut\n",
    "- create fake shortcuts and real one as pos and neg samples\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-Levenshtein\n",
      "  Downloading python_Levenshtein-0.21.1-py3-none-any.whl (9.4 kB)\n",
      "Collecting Levenshtein==0.21.1\n",
      "  Downloading Levenshtein-0.21.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (173 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.3/173.3 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting rapidfuzz<4.0.0,>=2.3.0\n",
      "  Downloading rapidfuzz-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: rapidfuzz, Levenshtein, python-Levenshtein\n",
      "Successfully installed Levenshtein-0.21.1 python-Levenshtein-0.21.1 rapidfuzz-3.2.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# !pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Levenshtein import distance, ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/19322 [00:13<25:02:01,  4.66s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:7\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/indexing.py:967\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    964\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    966\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[0;32m--> 967\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/indexing.py:1522\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1519\u001b[0m \u001b[38;5;66;03m# validate the location\u001b[39;00m\n\u001b[1;32m   1520\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_integer(key, axis)\n\u001b[0;32m-> 1522\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ixs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/frame.py:3424\u001b[0m, in \u001b[0;36mDataFrame._ixs\u001b[0;34m(self, i, axis)\u001b[0m\n\u001b[1;32m   3422\u001b[0m \u001b[38;5;66;03m# irow\u001b[39;00m\n\u001b[1;32m   3423\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 3424\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfast_xs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3426\u001b[0m     \u001b[38;5;66;03m# if we are a copy, mark as such\u001b[39;00m\n\u001b[1;32m   3427\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(new_values, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;129;01mand\u001b[39;00m new_values\u001b[38;5;241m.\u001b[39mbase \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/internals/managers.py:998\u001b[0m, in \u001b[0;36mBlockManager.fast_xs\u001b[0;34m(self, loc)\u001b[0m\n\u001b[1;32m    995\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    996\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39miget((\u001b[38;5;28mslice\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m), loc))\n\u001b[0;32m--> 998\u001b[0m dtype \u001b[38;5;241m=\u001b[39m \u001b[43minterleaved_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mblk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1000\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1001\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, ExtensionDtype):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# %%time\n",
    "# from tqdm import tqdm\n",
    "# d = a\n",
    "# similarities = np.zeros((d.shape[0], d.shape[0]))\n",
    "\n",
    "# for i in tqdm(range(4314,len(d))):\n",
    "#     for j in range(len(d)):\n",
    "#         similarities[i][j] = ratio(a.iloc[i]['norm'], b.iloc[j]['norm'] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(\"data/sim2.npy\", similarities[4314:15102])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# neg_dict = {'word':[], 'neg':[]} \n",
    "# for i,sim in enumerate(similarities):\n",
    "#     sorted = np.argsort(sim,axis=0)\n",
    "#     top_sim = sorted[::-1][1:20]\n",
    "#     top_sim = top_sim[np.where(sim[top_sim] >= 0.7)]\n",
    "#     word = a.iloc[i]['name']\n",
    "#     sim_neg = b.iloc[top_sim]['name'].tolist() \n",
    "#     neg_dict['word'] += [word]*len(sim_neg)\n",
    "\n",
    "#     neg_dict['neg'] += sim_neg\n",
    "\n",
    "\n",
    "\n",
    "# pd.DataFrame(neg_dict).to_json('data/neg_names.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neg_name = pd.read_json('data/neg_names.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'neg_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [33]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m neg_name[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnorm\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mneg_name\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x :re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw)([A-Z])\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m1 \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([s\u001b[38;5;241m.\u001b[39mcapitalize() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(s) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m s \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m x\u001b[38;5;241m.\u001b[39msplit()])))\\\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x : x\u001b[38;5;241m.\u001b[39mlower())\u001b[38;5;241m.\u001b[39mapply(preprocess)\n\u001b[1;32m      4\u001b[0m neg_name[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnorm_ws\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m neg_name[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnorm\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x : ntx\u001b[38;5;241m.\u001b[39mreplace_term(x, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m) ) \n\u001b[1;32m      6\u001b[0m neg_name[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneg_norm\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m neg_name[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneg\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x :re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw)([A-Z])\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m1 \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([s\u001b[38;5;241m.\u001b[39mcapitalize() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(s) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m s \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m x\u001b[38;5;241m.\u001b[39msplit()])))\\\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x : x\u001b[38;5;241m.\u001b[39mlower())\u001b[38;5;241m.\u001b[39mapply(preprocess)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'neg_name' is not defined"
     ]
    }
   ],
   "source": [
    "neg_name['norm'] = neg_name['word'].apply(lambda x :re.sub(r\"(\\w)([A-Z])\", r\"\\1 \\2\", ' '.join([s.capitalize() if len(s) < 5 else s for s in x.split()])))\\\n",
    "    .apply(lambda x : x.lower()).apply(preprocess)\n",
    "\n",
    "neg_name['norm_ws'] = neg_name['norm'].apply(lambda x : ntx.replace_term(x, ' ','') ) \n",
    "\n",
    "neg_name['neg_norm'] = neg_name['neg'].apply(lambda x :re.sub(r\"(\\w)([A-Z])\", r\"\\1 \\2\", ' '.join([s.capitalize() if len(s) < 5 else s for s in x.split()])))\\\n",
    "    .apply(lambda x : x.lower()).apply(preprocess)\n",
    "\n",
    "neg_name['neg_norm_ws'] = neg_name['neg_norm'].apply(lambda x : ntx.replace_term(x, ' ','') ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "positives = list(zip(a.norm_ws.tolist(),b.norm_ws.tolist()))\n",
    "negatives = list(zip(neg_name.norm_ws.tolist(),neg_name.neg_norm_ws.tolist() ) )\n",
    "\n",
    "\n",
    "X_train = positives + negatives\n",
    "y_train = [1]*len(positives) + [0]*len(negatives)\n",
    "\n",
    "assert len(X_train) == len(y_train)\n",
    "\n",
    "pd.DataFrame({\"X_train\":X_train,'y_train':y_train} ).to_json('data/training_data_v1.json')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle = True \n",
    "train_data = pd.read_json('data/training_data_v1.json')\n",
    "if shuffle: \n",
    "    train_data = train_data.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X_train</th>\n",
       "      <th>y_train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>55971</th>\n",
       "      <td>[bulova corp, lion corp]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3300</th>\n",
       "      <td>[boston college, boston college]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42383</th>\n",
       "      <td>[ariva ehf, artisan ehf]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28607</th>\n",
       "      <td>[att inc,  oath inc]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21216</th>\n",
       "      <td>[toll bros inc, toll brors inc]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15318</th>\n",
       "      <td>[oasis power llc, oasis power llc]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18740</th>\n",
       "      <td>[saudi fooddrug authority, saudi fooddrug auth...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21223</th>\n",
       "      <td>[tommy hilfiger corp, tommy hilfiger corp]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47718</th>\n",
       "      <td>[bgc llc, vtc llc]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8718</th>\n",
       "      <td>[galleria 400 llc, galleria 400 llc]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62673 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 X_train  y_train\n",
       "55971                           [bulova corp, lion corp]        0\n",
       "3300                    [boston college, boston college]        1\n",
       "42383                           [ariva ehf, artisan ehf]        0\n",
       "28607                               [att inc,  oath inc]        0\n",
       "21216                    [toll bros inc, toll brors inc]        1\n",
       "...                                                  ...      ...\n",
       "15318                 [oasis power llc, oasis power llc]        1\n",
       "18740  [saudi fooddrug authority, saudi fooddrug auth...        1\n",
       "21223         [tommy hilfiger corp, tommy hilfiger corp]        1\n",
       "47718                                 [bgc llc, vtc llc]        0\n",
       "8718                [galleria 400 llc, galleria 400 llc]        1\n",
       "\n",
       "[62673 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X_train</th>\n",
       "      <th>y_train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35031</th>\n",
       "      <td>[alleteinc, atexinc]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33542</th>\n",
       "      <td>[airgasinc, arazuinc]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3712</th>\n",
       "      <td>[cedromsniinc, cedromsniinc]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21695</th>\n",
       "      <td>[upmcchildren'shospitalofpittsburgh, upmcchildren'shospitalofpittsburgh]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53328</th>\n",
       "      <td>[bitzioinc, itroninc]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18194</th>\n",
       "      <td>[sdc, sdc]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12760</th>\n",
       "      <td>[mddsrl, mddsrl]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38964</th>\n",
       "      <td>[amrepcorp, arielcorp]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6812</th>\n",
       "      <td>[essabancorpinc, essabancorpinc]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11539</th>\n",
       "      <td>[kochenergyservicesllc, kochenergyservicesllc]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13698</th>\n",
       "      <td>[microsourceinc, microsourceinc]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61557</th>\n",
       "      <td>[camberenergyinc, srcenergyinc]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10771</th>\n",
       "      <td>[innolighttechnologycorp, innolighttechnologycorp]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17358</th>\n",
       "      <td>[rjreynoldstobaccoco, rjreynoldstobaccoco]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55913</th>\n",
       "      <td>[buildersfirstsourceinc, businessfirstbancsharesinc]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32045</th>\n",
       "      <td>[aegonthtflifeinsurancecoltd, nationalinsurancecorpltd]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3992</th>\n",
       "      <td>[cplvpropertyownerllc, cplvpropertyownerllc]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21863</th>\n",
       "      <td>[unionfidelitylifeinsuranceco, unionfidelitylifeinsuranceco]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40411</th>\n",
       "      <td>[apexsystemsllc, pegasystemsinc]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27825</th>\n",
       "      <td>[arinetworkservicesinc, basicenergyservicesinc]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                        X_train  \\\n",
       "35031                                                      [alleteinc, atexinc]   \n",
       "33542                                                     [airgasinc, arazuinc]   \n",
       "3712                                               [cedromsniinc, cedromsniinc]   \n",
       "21695  [upmcchildren'shospitalofpittsburgh, upmcchildren'shospitalofpittsburgh]   \n",
       "53328                                                     [bitzioinc, itroninc]   \n",
       "18194                                                                [sdc, sdc]   \n",
       "12760                                                          [mddsrl, mddsrl]   \n",
       "38964                                                    [amrepcorp, arielcorp]   \n",
       "6812                                           [essabancorpinc, essabancorpinc]   \n",
       "11539                            [kochenergyservicesllc, kochenergyservicesllc]   \n",
       "13698                                          [microsourceinc, microsourceinc]   \n",
       "61557                                           [camberenergyinc, srcenergyinc]   \n",
       "10771                        [innolighttechnologycorp, innolighttechnologycorp]   \n",
       "17358                                [rjreynoldstobaccoco, rjreynoldstobaccoco]   \n",
       "55913                      [buildersfirstsourceinc, businessfirstbancsharesinc]   \n",
       "32045                   [aegonthtflifeinsurancecoltd, nationalinsurancecorpltd]   \n",
       "3992                               [cplvpropertyownerllc, cplvpropertyownerllc]   \n",
       "21863              [unionfidelitylifeinsuranceco, unionfidelitylifeinsuranceco]   \n",
       "40411                                          [apexsystemsllc, pegasystemsinc]   \n",
       "27825                           [arinetworkservicesinc, basicenergyservicesinc]   \n",
       "\n",
       "       y_train  \n",
       "35031        0  \n",
       "33542        0  \n",
       "3712         1  \n",
       "21695        1  \n",
       "53328        0  \n",
       "18194        1  \n",
       "12760        1  \n",
       "38964        0  \n",
       "6812         1  \n",
       "11539        1  \n",
       "13698        1  \n",
       "61557        0  \n",
       "10771        1  \n",
       "17358        1  \n",
       "55913        0  \n",
       "32045        0  \n",
       "3992         1  \n",
       "21863        1  \n",
       "40411        0  \n",
       "27825        0  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_rows = 100\n",
    "pd.options.display.max_colwidth = None\n",
    "\n",
    "train_data.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModel\n",
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# #Mean Pooling - Take attention mask into account for correct averaging\n",
    "# def mean_pooling(model_output, attention_mask):\n",
    "#     token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "#     input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "#     return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "# # Sentences we want sentence embeddings for\n",
    "# sentences = ['This is an example sentence', 'Each sentence is converted']\n",
    "\n",
    "# # Load model from HuggingFace Hub\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# # Tokenize sentences\n",
    "# encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# # Compute token embeddings\n",
    "# with torch.no_grad():\n",
    "#     model_output = model(**encoded_input)\n",
    "\n",
    "# # Perform pooling\n",
    "# sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "# # Normalize embeddings\n",
    "# sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "95de5cd13349eda40311ab1dff18cd2882fe2f653551ced8d6bd11d5ebdaebd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
